{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# WEEK-01\nLast week, we learned how to code in Python and extract data from websites.\n\nData is like fuel for a car‚Äîwithout it, there is no AI. But having data alone isn‚Äôt enough; we need to understand, clean, and prepare it before training our models.\n\nThis week, we‚Äôll explore new tools and concepts, so be patient and make the most of this learning experience! üöÄ\nand remeber\n\n\n**5 minutes of reading can save you 6 hours of debugging**\n\n\nPlease use the dataset included in the subject. You can find it in the [GitHub repository](https://github.com/1337-Artificial-Intelligence/Entry-Level-ML-Engineer-Bootcamp/tree/main/week01/Housing.csv).\n\nGood luck! ü§û\n\n\n\n---\n\n",
      "metadata": {
        "id": "-F8jP57Uybp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise-01: Load and Inspect the Dataset\n\nThe goal of this exercise is to load a dataset and understand its structure.\n\nWe encourage you to use the Pandas library for this task.\nTo get started, explore the Pandas library by checking its [official documentation:](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html)\n\n### **Instructions**\n1. Load the dataset into a Pandas DataFrame.\n2. Print the number of rows in the dataset.\n\n\n### **Open Question**\n\n>üôã*What is a DataFrame, and why do we need it?*\n",
      "metadata": {
        "id": "amOY5m_1wd9K"
      }
    },
    {
      "cell_type": "code",
      "source": "# add your code here\nimport pandas as pd\ndf = pd.read_csv('Housing.csv')\nprint(len(df.index))",
      "metadata": {
        "id": "oy6SDy0MJ4UO",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "559\n"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": "The output should be:\n\n`Number of rows in the dataset: 559`",
      "metadata": {
        "id": "rc_zlJI-MuPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\nNow that we have successfully loaded the dataset and counted the number of rows, let's display it.\n\nHowever, when working with large datasets, it's not practical to display everything at once. Instead, let's display only the first 5 rows to get a quick overview! üöÄ",
      "metadata": {
        "id": "Da0JHHbFOEaD"
      }
    },
    {
      "cell_type": "code",
      "source": "# add your code here\nprint(df.iloc[0:5])",
      "metadata": {
        "id": "a0SjfYiwQ9hU",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "        price    area  bedrooms  bathrooms  stories mainroad guestroom  \\\n0  13300000.0  7420.0         4          2      3.0      yes        no   \n1  12250000.0  8960.0         4          4      4.0      yes        no   \n2  12250000.0  9960.0         3          2      2.0      yes        no   \n3  12215000.0  7500.0         4          2      2.0      yes        no   \n4  11410000.0  7420.0         4          1      2.0      yes       yes   \n\n  basement hotwaterheating airconditioning  parking prefarea furnishingstatus  \n0       no              no             yes      2.0      yes        furnished  \n1       no              no             yes      3.0       no        furnished  \n2      yes              no              no      2.0      yes   semi-furnished  \n3      yes              no             yes      3.0      yes        furnished  \n4      yes              no             yes      2.0       no        furnished  \n"
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": "The output should be like this\n\n\n\n```\n\tprice\tarea\tbedrooms\tbathrooms\tstories\tmainroad\tguestroom\tbasement\thotwaterheating\tairconditioning\tparking\tprefarea\tfurnishingstatus\n0\t13300000.0\t7420.0\t4\t2\t3.0\tyes\tno\tno\tno\tyes\t2.0\tyes\tfurnished\n1\t12250000.0\t8960.0\t4\t4\t4.0\tyes\tno\tno\tno\tyes\t3.0\tno\tfurnished\n2\t12250000.0\t9960.0\t3\t2\t2.0\tyes\tno\tyes\tno\tno\t2.0\tyes\tsemi-furnished\n3\t12215000.0\t7500.0\t4\t2\t2.0\tyes\tno\tyes\tno\tyes\t3.0\tyes\tfurnished\n4\t11410000.0\t7420.0\t4\t1\t2.0\tyes\tyes\tyes\tno\tyes\t2.0\tno\tfurnished\n\n```\n\n",
      "metadata": {
        "id": "MYsrJY6LUqAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\nNow that we've loaded the dataset and displayed the first few rows, let's dive deeper and understand its structure.\nTo do this, use a function that provides essential details about the dataset, such as:\n\n- The number of non-null values in each column\n- The data types of each column\n- The total number of entries\n\n",
      "metadata": {
        "id": "XMgqqFZPWhXA"
      }
    },
    {
      "cell_type": "code",
      "source": "# add your code here\nprint(df.info())",
      "metadata": {
        "id": "90JrJ09PwW7n",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 559 entries, 0 to 558\nData columns (total 13 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   price             558 non-null    float64\n 1   area              558 non-null    float64\n 2   bedrooms          559 non-null    int64  \n 3   bathrooms         559 non-null    int64  \n 4   stories           557 non-null    float64\n 5   mainroad          559 non-null    object \n 6   guestroom         559 non-null    object \n 7   basement          559 non-null    object \n 8   hotwaterheating   559 non-null    object \n 9   airconditioning   557 non-null    object \n 10  parking           558 non-null    float64\n 11  prefarea          558 non-null    object \n 12  furnishingstatus  557 non-null    object \ndtypes: float64(4), int64(2), object(7)\nmemory usage: 41.6+ KB\nNone\n"
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "markdown",
      "source": "Output format\n\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 559 entries, 0 to 558\nData columns (total 13 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   price             558 non-null    float64\n 1   area              558 non-null    float64\n 2   bedrooms          559 non-null    int64  \n 3   bathrooms         559 non-null    int64  \n 4   stories           557 non-null    float64\n 5   mainroad          559 non-null    object\n 6   guestroom         559 non-null    object\n 7   basement          559 non-null    object\n 8   hotwaterheating   559 non-null    object\n 9   airconditioning   557 non-null    object\n 10  parking           558 non-null    float64\n 11  prefarea          558 non-null    object\n 12  furnishingstatus  557 non-null    object\ndtypes: float64(4), int64(2), object(7)\nmemory usage: 56.9+ KB\n```\n\n",
      "metadata": {
        "id": "Z9O4aFINbP19"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---",
      "metadata": {
        "id": "a03bL1o8clI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Before we start processing the dataset, it's important to check if there are any missing values. Missing data can affect model performance and lead to inaccurate results, so identifying and handling them is a key step in data preparation.\n\nTry using a method that allows you to see how many missing values exist in each column.\n",
      "metadata": {
        "id": "pvc94RSEJ5EG"
      }
    },
    {
      "cell_type": "code",
      "source": "# add your code here\nprint(df.isnull().sum())",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePnnjw1AALmR",
        "outputId": "e9eb2d4b-2db1-40e0-f159-bc600b9e87c3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "price               1\narea                1\nbedrooms            0\nbathrooms           0\nstories             2\nmainroad            0\nguestroom           0\nbasement            0\nhotwaterheating     0\nairconditioning     2\nparking             1\nprefarea            1\nfurnishingstatus    2\ndtype: int64\n"
        }
      ],
      "execution_count": 36
    },
    {
      "cell_type": "markdown",
      "source": "Output format\n\n\n```\nprice               1\narea                1\nbedrooms            0\nbathrooms           0\nstories             2\nmainroad            0\nguestroom           0\nbasement            0\nhotwaterheating     0\nairconditioning     2\nparking             1\nprefarea            1\nfurnishingstatus    2\ndtype: int64\n```\n\n",
      "metadata": {
        "id": "UNmdK-q4uF0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n# Exercise 2: Data Cleaning\n\nNow that we've identified missing values and inspected our dataset, it's time to clean the data!\n\nIn this exercise, you'll focus on:\n\nRemoving or handling missing values\nIdentifying and removing duplicate rows\nCleaning data is a crucial step to ensure our model is trained on high-quality, reliable data.\n### üôã Open Question\n> Why is it important to remove duplicate rows in a dataset?",
      "metadata": {
        "id": "97zz0o_dyWLk"
      }
    },
    {
      "cell_type": "code",
      "source": "# add your code here\nimport pandas as pd\n\ndf = pd.read_csv('Housing.csv')\ncleaned_df = df.dropna() # -> for dropping any NA values\ncleaned_df = cleaned_df.drop_duplicates() # -> for removing duplicates\nprint(cleaned_df)",
      "metadata": {
        "id": "SNPcxYR-xOUe",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "          price    area  bedrooms  bathrooms  stories mainroad guestroom  \\\n0    13300000.0  7420.0         4          2      3.0      yes        no   \n1    12250000.0  8960.0         4          4      4.0      yes        no   \n2    12250000.0  9960.0         3          2      2.0      yes        no   \n3    12215000.0  7500.0         4          2      2.0      yes        no   \n4    11410000.0  7420.0         4          1      2.0      yes       yes   \n..          ...     ...       ...        ...      ...      ...       ...   \n552   1820000.0  3000.0         2          1      1.0      yes        no   \n553   1767150.0  2400.0         3          1      1.0       no        no   \n554   1750000.0  3620.0         2          1      1.0      yes        no   \n555   1750000.0  2910.0         3          1      1.0       no        no   \n556   1750000.0  3850.0         3          1      2.0      yes        no   \n\n    basement hotwaterheating airconditioning  parking prefarea  \\\n0         no              no             yes      2.0      yes   \n1         no              no             yes      3.0       no   \n2        yes              no              no      2.0      yes   \n3        yes              no             yes      3.0      yes   \n4        yes              no             yes      2.0       no   \n..       ...             ...             ...      ...      ...   \n552      yes              no              no      2.0       no   \n553       no              no              no      0.0       no   \n554       no              no              no      0.0       no   \n555       no              no              no      0.0       no   \n556       no              no              no      0.0       no   \n\n    furnishingstatus  \n0          furnished  \n1          furnished  \n2     semi-furnished  \n3          furnished  \n4          furnished  \n..               ...  \n552      unfurnished  \n553   semi-furnished  \n554      unfurnished  \n555        furnished  \n556      unfurnished  \n\n[544 rows x 13 columns]\n"
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": "The output should be\n\n\n```\n\nprice\tarea\tbedrooms\tbathrooms\tstories\tmainroad\tguestroom\tbasement\thotwaterheating\tairconditioning\tparking\tprefarea\tfurnishingstatus\n0\t13300000.0\t7420.0\t4\t2\t3.0\tyes\tno\tno\tno\tyes\t2.0\tyes\tfurnished\n1\t12250000.0\t8960.0\t4\t4\t4.0\tyes\tno\tno\tno\tyes\t3.0\tno\tfurnished\n2\t12250000.0\t9960.0\t3\t2\t2.0\tyes\tno\tyes\tno\tno\t2.0\tyes\tsemi-furnished\n3\t12215000.0\t7500.0\t4\t2\t2.0\tyes\tno\tyes\tno\tyes\t3.0\tyes\tfurnished\n4\t11410000.0\t7420.0\t4\t1\t2.0\tyes\tyes\tyes\tno\tyes\t2.0\tno\tfurnished\n...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n547\t1820000.0\t3000.0\t2\t1\t1.0\tyes\tno\tyes\tno\tno\t2.0\tno\tunfurnished\n548\t1767150.0\t2400.0\t3\t1\t1.0\tno\tno\tno\tno\tno\t0.0\tno\tsemi-furnished\n549\t1750000.0\t3620.0\t2\t1\t1.0\tyes\tno\tno\tno\tno\t0.0\tno\tunfurnished\n550\t1750000.0\t2910.0\t3\t1\t1.0\tno\tno\tno\tno\tno\t0.0\tno\tfurnished\n551\t1750000.0\t3850.0\t3\t1\t2.0\tyes\tno\tno\tno\tno\t0.0\tno\tunfurnished\n544 rows √ó 13 columns```\n\n",
      "metadata": {
        "id": "d0d8POeJweu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n# Exercice 3 : Descriptive Statistics\n\nNow that we have cleaned our dataset, it's time to explore and understand it better using **descriptive statistics**. These statistics help summarize the data and give us insights into its distribution, central tendency, and variability.  \n\n### **Your Tasks**  \n1. Use Pandas to calculate basic statistics such as:  \n   - Mean, median, and standard deviation of numerical columns.  \n   - Minimum and maximum values.  \n   - Count of non-null values in each column.\n2. Check the distribution of numerical features\n\n\n3.  Show unique values for categorical variables.\n\n\n### üôã Open Question\n> Why do we need descriptive statistics when working with data?\n\nüöÄ **Let‚Äôs analyze our dataset!**\n\n###1. Calculate basic statistics",
      "metadata": {
        "id": "2QB2b_pXzsr8"
      }
    },
    {
      "cell_type": "code",
      "source": "# add your code here\nimport pandas as pd\n\ndf = pd.read_csv('Housing.csv')\nprint(df.describe())",
      "metadata": {
        "id": "Nf8yRGgP3CD5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "1.330000e+07\n              price          area    bedrooms   bathrooms     stories  \\\ncount  5.580000e+02    558.000000  559.000000  559.000000  557.000000   \nmean   4.777083e+06   5135.637993    2.967800    1.288014    1.811490   \nstd    1.885783e+06   2157.286148    0.734969    0.502020    0.872446   \nmin    1.750000e+06   1650.000000    1.000000    1.000000    1.000000   \n25%    3.438750e+06   3600.000000    2.500000    1.000000    1.000000   \n50%    4.340000e+06   4515.000000    3.000000    1.000000    2.000000   \n75%    5.808250e+06   6360.000000    3.000000    2.000000    2.000000   \nmax    1.330000e+07  16200.000000    6.000000    4.000000    4.000000   \n\n          parking  \ncount  558.000000  \nmean     0.691756  \nstd      0.858982  \nmin      0.000000  \n25%      0.000000  \n50%      0.000000  \n75%      1.000000  \nmax      3.000000  \n"
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": "*Your* output should be in this format:\n\n\n```\nBasic Statistics:\n              price          area    bedrooms   bathrooms     stories  \\\ncount  5.580000e+02    558.000000  559.000000  559.000000  557.000000   \nmean   4.777083e+06   5135.637993    2.967800    1.288014    1.811490   \nstd    1.885783e+06   2157.286148    0.734969    0.502020    0.872446   \nmin    1.750000e+06   1650.000000    1.000000    1.000000    1.000000   \n25%    3.438750e+06   3600.000000    2.500000    1.000000    1.000000   \n50%    4.340000e+06   4515.000000    3.000000    1.000000    2.000000   \n75%    5.808250e+06   6360.000000    3.000000    2.000000    2.000000   \nmax    1.330000e+07  16200.000000    6.000000    4.000000    4.000000   \n\n          parking  \ncount  558.000000  \nmean     0.691756  \nstd      0.858982  \nmin      0.000000  \n25%      0.000000  \n50%      0.000000  \n75%      1.000000  \nmax      3.000000  \n\n```\n\n",
      "metadata": {
        "id": "UUTFgZ5a0aBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": "###2.Check the distribution of numerical features using:  \n",
      "metadata": {
        "id": "Y27XCWRl4z1G"
      }
    },
    {
      "cell_type": "code",
      "source": "# add your code here\nimport pandas as pd\n\ndf = pd.read_csv('Housing.csv')\nnumerical_df = df.select_dtypes(exclude=['number'])\nprint('Distribution of numerical features\\n')\nfor col in numerical_df.columns:\n    print('Value Counts for {col}'.format(col=col))\n    print(df[col].value_counts())\n    print()",
      "metadata": {
        "id": "-D8h4wFL418J",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Distribution of numerical features\n\nValue Counts for mainroad\nmainroad\nyes    481\nno      78\nName: count, dtype: int64\n\nValue Counts for guestroom\nguestroom\nno     460\nyes     99\nName: count, dtype: int64\n\nValue Counts for basement\nbasement\nno     365\nyes    194\nName: count, dtype: int64\n\nValue Counts for hotwaterheating\nhotwaterheating\nno     533\nyes     26\nName: count, dtype: int64\n\nValue Counts for airconditioning\nairconditioning\nno     382\nyes    175\nName: count, dtype: int64\n\nValue Counts for prefarea\nprefarea\nno     425\nyes    133\nName: count, dtype: int64\n\nValue Counts for furnishingstatus\nfurnishingstatus\nsemi-furnished    230\nunfurnished       183\nfurnished         144\nName: count, dtype: int64\n\n"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": "Your output should be in this format:\n\n```\nDistribution of numerical features\n\nValue Counts for mainroad:\nmainroad\nyes    481\nno      78\nName: count, dtype: int64\n\nValue Counts for guestroom:\nguestroom\nno     460\nyes     99\nName: count, dtype: int64\n\nValue Counts for basement:\nbasement\nno     365\nyes    194\nName: count, dtype: int64\n\nValue Counts for hotwaterheating:\nhotwaterheating\nno     533\nyes     26\nName: count, dtype: int64\n\nValue Counts for airconditioning:\nairconditioning\nno     382\nyes    175\nName: count, dtype: int64\n\nValue Counts for prefarea:\nprefarea\nno     425\nyes    133\nName: count, dtype: int64\n\nValue Counts for furnishingstatus:\nfurnishingstatus\nsemi-furnished    230\nunfurnished       183\nfurnished         144\nName: count, dtype: int64\n```\n\n",
      "metadata": {
        "id": "IGqaMGCZ5BA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": "###3.Show unique values for categorical variables.\n",
      "metadata": {
        "id": "PvAoHyxM02vq"
      }
    },
    {
      "cell_type": "code",
      "source": "# add your code here\nfor col in numerical_df:\n    print('Unique values in \\'{col}\\':'.format(col=col))\n    print(numerical_df[col].unique())\n    print()",
      "metadata": {
        "id": "lfkJvsAJ0h4G",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Unique values in 'mainroad':\n['yes' 'no']\n\nUnique values in 'guestroom':\n['no' 'yes']\n\nUnique values in 'basement':\n['no' 'yes']\n\nUnique values in 'hotwaterheating':\n['no' 'yes']\n\nUnique values in 'airconditioning':\n['yes' 'no' nan]\n\nUnique values in 'prefarea':\n['yes' 'no' nan]\n\nUnique values in 'furnishingstatus':\n['furnished' 'semi-furnished' 'unfurnished' nan]\n\n"
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": "Output format\n\n\n```\nUnique values in 'mainroad':\n['yes' 'no']\n\nUnique values in 'guestroom':\n['no' 'yes']\n\nUnique values in 'basement':\n['no' 'yes']\n\nUnique values in 'hotwaterheating':\n['no' 'yes']\n\nUnique values in 'airconditioning':\n['yes' 'no' nan]\n\nUnique values in 'prefarea':\n['yes' 'no' nan]\n\nUnique values in 'furnishingstatus':\n['furnished' 'semi-furnished' 'unfurnished' nan]\n```\n\n",
      "metadata": {
        "id": "LRtso6Iz6QAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": "#Exercise 4: Univariate Analysis**  \n\nIn this exercise, you'll analyze the distribution of each feature using visualization techniques. Understanding how your data is distributed is crucial for identifying patterns, outliers, and potential preprocessing steps.  \n\n### **Tasks**  \n1Ô∏è‚É£ **Plot histograms** for numerical variables to see their distribution.  \n2Ô∏è‚É£ **Display boxplots** to detect outliers.  \n3Ô∏è‚É£ **Plot pie charts** for categorical variables to understand their proportions.  \n\nüí° **Hint**: Use **Matplotlib** and **Seaborn** for plotting. You can explore their documentation here:  \n- [Matplotlib Docs](https://matplotlib.org/)  \n- [Seaborn Docs](https://seaborn.pydata.org/)  \n\nHappy coding! üöÄ",
      "metadata": {
        "id": "D5J64ijJ1-5n"
      }
    },
    {
      "cell_type": "code",
      "source": "!pip install seaborn",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'OSError'>",
          "evalue": "Not available",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip install seaborn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2653\u001b[0m, in \u001b[0;36mInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2648\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[1;32m   2651\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[1;32m   2652\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[0;32m-> 2653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/IPython/utils/_process_emscripten.py:11\u001b[0m, in \u001b[0;36msystem\u001b[0;34m(cmd)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msystem\u001b[39m(cmd):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mOSError\u001b[0m: Not available"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "# add your code here\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf.select_dtypes(include='number').hist(bins=20, figsize=(10, 8))\nplt.suptitle(\"Histograms of Numerical Variables\")\nplt.tight_layout()\nplt.show()\n# numeric_cols = df.select_dtypes(include='number').columns\n\n# plt.figure(figsize=(12, 6))\n# for i, col in enumerate(numeric_cols):\n#     plt.subplot(1, len(numeric_cols), i + 1)\n#     sns.boxplot(y=df[col])\n#     plt.title(col)\n# plt.tight_layout()\n# plt.show()",
      "metadata": {
        "id": "R03Q6GyC2JJi",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Matplotlib is building the font cache; this may take a moment.\n"
        },
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'seaborn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# add your code here\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mhist(bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistograms of Numerical Variables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n\n**Learning takes time! If things feel challenging, that's completely normal. Be patient, keep exploring, and trust the process‚Äîyou‚Äôre making progress!**\n\nüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ",
      "metadata": {
        "id": "X-j1RDSk_D6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n#Exercise 5: Bivariate Analysis\nNow that we have explored individual features, let‚Äôs analyze the relationships between two variables. This will help us understand how different features interact with each other.\n\nTasks\n\n1Ô∏è‚É£ Scatter plots ‚Äì Visualize relationships between numerical variables.\n\n2Ô∏è‚É£ Correlation heatmap ‚Äì Identify correlations between numerical features.\n\n3Ô∏è‚É£ Boxplots ‚Äì Compare categorical and numerical variables.\n\nüí° Hint: Use Matplotlib and Seaborn for visualization.\n\nScatter plots help detect trends or patterns.\nThe heatmap shows how strongly two numerical variables are related.\nBoxplots help compare distributions across categories.\nLet‚Äôs dive in! üöÄ",
      "metadata": {
        "id": "sQc-34qD3h2q"
      }
    },
    {
      "cell_type": "code",
      "source": "# add your code here",
      "metadata": {
        "id": "aVfrYhJr3lYh",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n#Exercise 6: Multivariate Analysis\n\nNow that we've explored relationships between two variables, let's analyze interactions among multiple variables simultaneously. This will help us uncover deeper patterns in the data.\n\nTasks\n\n1Ô∏è‚É£ Pairplot ‚Äì Visualize relationships between multiple numerical variables.\n\n2Ô∏è‚É£ Multivariate correlation heatmap ‚Äì Identify correlations across multiple features.\n\n3Ô∏è‚É£ Grouped boxplots ‚Äì Compare distributions of a numerical variable across multiple categorical variables.\n\nüí° Hint:\n\nUse Seaborn's pairplot to explore all pairwise relationships.\n\nThe heatmap helps identify groups of strongly correlated variables.\n\nGrouped boxplots provide insights into variations across different categories.",
      "metadata": {
        "id": "rEtLuNPvQppw"
      }
    },
    {
      "cell_type": "code",
      "source": "# add your code here",
      "metadata": {
        "id": "uj0OkEkmQt4Y",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n#Exercise 7: Making new features\n\nFeature engineering is a powerful way to improve model performance by adding meaningful information that the machine can't detect on its own.  \n\n### **Tasks**  \n1Ô∏è‚É£ Create **one new feature** based on existing columns.  \n   - You can **combine** or **split** columns.  \n   - Extract meaningful insights (e.g., **datetime ‚Üí morning/evening**, **weekend/weekday**, **text length**, etc.).  \n\n2Ô∏è‚É£ Add these new features to your dataset.  \n\n\n3Ô∏è‚É£ Display the first few rows to check the results.  \n\n\n\n### üôã Open Question\n\n> How do new features impact machine learning models? Can you think of a real-world example where feature engineering made a big difference?\n\nGet creative and enhance your dataset! üöÄ",
      "metadata": {
        "id": "lNqPT5KIRLoU"
      }
    },
    {
      "cell_type": "code",
      "source": "#add your code here",
      "metadata": {
        "id": "3rIvv4OcRNx3",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# **Exercise 8: Preparing Data for Machine Learning**  \n\nTo make our dataset machine learning-friendly, we need to **convert categorical variables into numerical values** and **scale numerical features** for better model performance.  \n\n### **Tasks**  \n1Ô∏è‚É£ **Convert ordinal categorical variables** into numerical values using **Label Encoding**.  \n   - Example: `(\"yes\",\"no\") ‚Üí (1,0)`  \n   - Example: `(\"very unsatisfied\", \"unsatisfied\", \"neutral\", \"satisfied\", \"very satisfied\") ‚Üí (0,1,2,3,4)`  \n\n2Ô∏è‚É£ **Apply One-Hot Encoding** for **nominal categorical variables** (where order doesn't matter).  \n\n3Ô∏è‚É£ **Scale numerical variables** using any scaler (MinMaxScaler, StandardScaler, etc.).  \n   - Choose the best scaler for your dataset and **justify your choice**.  \n\nüí° **Hint**:  \n- Use **Label Encoding** for categories with an inherent order.  \n- Use **OneHotEncoding** for categories without order (e.g., colors, cities).  \n- Scaling helps models **converge faster** and improves **performance**.  \n\n### üôã Open Question\n> Why do we need to scale numerical variables? Can you think of a case where scaling isn't necessary?\n\nLet's transform our dataset into ML-ready data! üöÄ",
      "metadata": {
        "id": "A3_CHQ-0RT4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n\n---\n\n## üéâ Congratulations! You've Completed Week01!  \n\nGreat job on making it through week 1 of the bootcamp! üöÄ You've learned how to load, clean, and explore data‚Äîessential skills for any ML engineer.  \n\nData preprocessing is a crucial step in any machine learning project, and mastering it will set you up for success in the coming weeks.  \n\nüîç **What‚Äôs Next?**  \nTake some time to review what you‚Äôve learned and apply it to different datasets. If you have any questions or got stuck along the way, don‚Äôt hesitate to ask in our community!  \n\nStay curious, keep experimenting, and get ready for Week 2! üí°üî•",
      "metadata": {
        "id": "dpZdO155_eRI"
      }
    }
  ]
}